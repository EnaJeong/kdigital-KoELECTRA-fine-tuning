{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "koelectra_ner.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMiXgp+g71AmwAh/LobEh3X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EnaJeong/study-KoELECTRA_fine_tuning/blob/main/koelectra_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTac4x38HiRV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b31f6b6-e81b-4e4c-a82b-2afb1c099e4b"
      },
      "source": [
        "!pip install Attrdict \n",
        "!pip install Transformers\n",
        "!pip install seqeval"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting seqeval\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/2d/233c79d5b4e5ab1dbf111242299153f3caddddbb691219f363ad55ce783d/seqeval-1.2.2.tar.gz (43kB)\n",
            "\r\u001b[K     |███████▌                        | 10kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 20kB 18.1MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 30kB 8.0MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 40kB 3.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 51kB 2.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.0.1)\n",
            "Building wheels for collected packages: seqeval\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-1.2.2-cp37-none-any.whl size=16172 sha256=2c6dc11278d0c4b5dc9c90c6e5a843f18533a2245cdabc0eed695c1fed1557a9\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/df/1b/45d75646c37428f7e626214704a0e35bd3cfc32eda37e59e5f\n",
            "Successfully built seqeval\n",
            "Installing collected packages: seqeval\n",
            "Successfully installed seqeval-1.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_AKxxCuG8vc"
      },
      "source": [
        "import argparse\n",
        "import json\n",
        "import logging\n",
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from fastprogress.fastprogress import master_bar, progress_bar\n",
        "from attrdict import AttrDict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCOMjm4FHmhV"
      },
      "source": [
        "from transformers import (AdamW,get_linear_schedule_with_warmup)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svfvYhV9Lm7W"
      },
      "source": [
        "# src/utils.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIAESuyrHvk0"
      },
      "source": [
        "# from src import (                  #src에서 import 하는 모듈을 자체 모듈로 정의\n",
        "#     CONFIG_CLASSES,\n",
        "#     TOKENIZER_CLASSES,\n",
        "#     MODEL_FOR_TOKEN_CLASSIFICATION,\n",
        "#     init_logger,\n",
        "#     set_seed,\n",
        "#     compute_metrics,\n",
        "#     show_ner_report\n",
        "# )\n",
        "from transformers import ElectraConfig, ElectraTokenizer, ElectraForTokenClassification  \n",
        "from seqeval import metrics as seqeval_metrics #평가 매트릭스 \n",
        "\n",
        "def init_logger():\n",
        "    logging.basicConfig(\n",
        "        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\", #?\n",
        "        datefmt=\"%m/%d/%Y %H:%M:%S\", #data 처리시간\n",
        "        level=logging.INFO,)\n",
        "\n",
        "def compute_metrics(labels, preds):\n",
        "    assert len(preds) == len(labels) #예측값과 결과값의 수가 같다면 return\n",
        "    \n",
        "    return {\n",
        "        \"precision\": seqeval_metrics.precision_score(labels, preds, suffix=True), \n",
        "        \"recall\": seqeval_metrics.recall_score(labels, preds, suffix=True), \n",
        "        \"f1\": seqeval_metrics.f1_score(labels, preds, suffix=True),\n",
        "    }\n",
        "\n",
        "def show_ner_report(labels, preds):\n",
        "    return seqeval_metrics.classification_report(labels, preds, suffix=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qcoix5HDLuAD"
      },
      "source": [
        "# from processor import ner_load_and_cache_examples as load_and_cache_examples\n",
        "# from processor import ner_tasks_num_labels as tasks_num_labels\n",
        "# from processor import ner_processors as processors\n",
        "\n",
        "from torch.utils.data import TensorDataset\n",
        "import copy\n",
        "\n",
        "def load_and_cache_examples(args, tokenizer, mode):\n",
        "    processor = PROCESSOR(args)\n",
        "    # Load data features from cache or dataset file\n",
        "    cached_features_file = os.path.join(\n",
        "        args.data_dir,\n",
        "        \"cached_{}_{}_{}_{}\".format(\n",
        "            str(args.task),\n",
        "            list(filter(None, args.model_name_or_path.split(\"/\"))).pop(), #배열의 마지막요소를 제거하고 그 요소를 반환\n",
        "            str(args.max_seq_len),\n",
        "            mode\n",
        "        )\n",
        "    )\n",
        "    if os.path.exists(cached_features_file):\n",
        "        logger.info(\"Loading features from cached file %s\", cached_features_file)\n",
        "        features = torch.load(cached_features_file)\n",
        "    else:\n",
        "        logger.info(\"Creating features from dataset file at %s\", args.data_dir)\n",
        "        if mode == \"train\":\n",
        "            examples = processor.get_examples(\"train\")\n",
        "        elif mode == \"dev\":\n",
        "            examples = processor.get_examples(\"dev\")\n",
        "        elif mode == \"test\":\n",
        "            examples = processor.get_examples(\"test\")\n",
        "        else:\n",
        "            raise ValueError(\"For mode, only train, dev, test is avaiable\")\n",
        "\n",
        "        pad_token_label_id = CrossEntropyLoss().ignore_index\n",
        "        features = ner_convert_examples_to_features( #전처리 과정\n",
        "            args,\n",
        "            examples,\n",
        "            tokenizer,\n",
        "            max_seq_length=args.max_seq_len,\n",
        "            task=args.task,\n",
        "            pad_token_label_id=pad_token_label_id\n",
        "        )\n",
        "        logger.info(\"Saving features into cached file %s\", cached_features_file)\n",
        "        torch.save(features, cached_features_file)\n",
        "\n",
        "    # Convert to Tensors and build dataset\n",
        "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
        "    all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n",
        "    all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n",
        "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
        "\n",
        "    dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_label_ids)\n",
        "    return dataset\n",
        "\n",
        "def ner_convert_examples_to_features( #전처리 함수\n",
        "        args,\n",
        "        examples,\n",
        "        tokenizer,\n",
        "        max_seq_length,\n",
        "        task,\n",
        "        pad_token_label_id=-100,\n",
        "):\n",
        "    label_lst = PROCESSOR(args).get_labels()\n",
        "    label_map = {label: i for i, label in enumerate(label_lst)} #label 맵핑\n",
        "\n",
        "    features = []\n",
        "    for (ex_index, example) in enumerate(examples):\n",
        "        if ex_index % 10000 == 0:\n",
        "            logger.info(\"Writing example {} of {}\".format(ex_index, len(examples)))\n",
        "\n",
        "        tokens = []\n",
        "        label_ids = []\n",
        "\n",
        "        for word, label in zip(example.words, example.labels):\n",
        "            word_tokens = tokenizer.tokenize(word) #BERT형식의 토큰화 진행\n",
        "            if not word_tokens:\n",
        "                word_tokens = [tokenizer.unk_token]  # For handling the bad-encoded word\n",
        "            tokens.extend(word_tokens)\n",
        "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
        "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
        "\n",
        "        special_tokens_count = 2  #SEP TOKENR /  CLS TOKEN\n",
        "        if len(tokens) > max_seq_length - special_tokens_count:\n",
        "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
        "            label_ids = label_ids[:(max_seq_length - special_tokens_count)]\n",
        "\n",
        "        # Add [SEP]\n",
        "        tokens += [tokenizer.sep_token]\n",
        "        label_ids += [pad_token_label_id]\n",
        "\n",
        "        # Add [CLS]\n",
        "        tokens = [tokenizer.cls_token] + tokens\n",
        "        label_ids = [pad_token_label_id] + label_ids\n",
        "\n",
        "        token_type_ids = [0] * len(tokens)\n",
        "\n",
        "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        attention_mask = [1] * len(input_ids)\n",
        "\n",
        "        padding_length = max_seq_length - len(input_ids)\n",
        "        input_ids += [tokenizer.pad_token_id] * padding_length\n",
        "        attention_mask += [0] * padding_length\n",
        "        token_type_ids += [0] * padding_length\n",
        "        label_ids += [pad_token_label_id] * padding_length\n",
        "\n",
        "        assert len(input_ids) == max_seq_length\n",
        "        assert len(attention_mask) == max_seq_length\n",
        "        assert len(token_type_ids) == max_seq_length\n",
        "        assert len(label_ids) == max_seq_length\n",
        "\n",
        "        if ex_index < 5:\n",
        "            logger.info(\"*** Example ***\")\n",
        "            logger.info(\"guid: %s\" % example.guid)\n",
        "            logger.info(\"tokens: %s\" % \" \".join([str(x) for x in tokens]))\n",
        "            logger.info(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
        "            logger.info(\"attention_mask: %s\" % \" \".join([str(x) for x in attention_mask]))\n",
        "            logger.info(\"token_type_ids: %s\" % \" \".join([str(x) for x in token_type_ids]))\n",
        "            logger.info(\"label: %s \" % \" \".join([str(x) for x in label_ids]))\n",
        "\n",
        "        features.append(\n",
        "            InputFeatures(input_ids=input_ids,\n",
        "                          attention_mask=attention_mask,\n",
        "                          token_type_ids=token_type_ids,\n",
        "                          label_ids=label_ids)\n",
        "        )\n",
        "    return features\n",
        "\n",
        "class InputFeatures(object):\n",
        "    \"\"\"A single set of features of data.\"\"\"\n",
        "\n",
        "    def __init__(self, input_ids, attention_mask, token_type_ids, label_ids):\n",
        "        self.input_ids = input_ids\n",
        "        self.attention_mask = attention_mask\n",
        "        self.token_type_ids = token_type_ids\n",
        "        self.label_ids = label_ids\n",
        "\n",
        "    def __repr__(self):\n",
        "        return str(self.to_json_string())\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Serializes this instance to a Python dictionary.\"\"\"\n",
        "        output = copy.deepcopy(self.__dict__)\n",
        "        return output\n",
        "\n",
        "    def to_json_string(self):\n",
        "        \"\"\"Serializes this instance to a JSON string.\"\"\"\n",
        "        return json.dumps(self.to_dict(), indent=2, sort_keys=True) + \"\\n\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cimH8t-hG2Ea"
      },
      "source": [
        "class NaverNerProcessor(object):\n",
        "    \"\"\"Processor for the Naver NER data set \"\"\"\n",
        "\n",
        "    def __init__(self, args):\n",
        "        self.args = args\n",
        "\n",
        "    def get_labels(self):\n",
        "        return [\"O\",\n",
        "                \"PER-B\", \"PER-I\", \"FLD-B\", \"FLD-I\", \"AFW-B\", \"AFW-I\", \"ORG-B\", \"ORG-I\",\n",
        "                \"LOC-B\", \"LOC-I\", \"CVL-B\", \"CVL-I\", \"DAT-B\", \"DAT-I\", \"TIM-B\", \"TIM-I\",\n",
        "                \"NUM-B\", \"NUM-I\", \"EVT-B\", \"EVT-I\", \"ANM-B\", \"ANM-I\", \"PLT-B\", \"PLT-I\",\n",
        "                \"MAT-B\", \"MAT-I\", \"TRM-B\", \"TRM-I\"]\n",
        "\n",
        "                #PER:인물 / FLD:학문분야 / AFW:인공물/ ORG:기관 등등..B=begin, I = in\n",
        "\n",
        "    @classmethod\n",
        "    def _read_file(cls, input_file):\n",
        "        \"\"\"Read tsv file, and return words and label as list\"\"\"\n",
        "        with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "            lines = []\n",
        "            for line in f:\n",
        "                lines.append(line.strip())\n",
        "            return lines\n",
        "\n",
        "    def _create_examples(self, dataset, set_type):\n",
        "        \"\"\"Creates examples for the training and dev sets.\"\"\"\n",
        "        examples = []\n",
        "        for (i, data) in enumerate(dataset):\n",
        "            words, labels = data.split('\\t')\n",
        "            words = words.split()\n",
        "            labels = labels.split()\n",
        "            guid = \"%s-%s\" % (set_type, i) #전역고유식별자 생성\n",
        "\n",
        "            assert len(words) == len(labels)\n",
        "\n",
        "            if i % 10000 == 0:\n",
        "                logger.info(data)\n",
        "            examples.append(InputExample(guid=guid, words=words, labels=labels))\n",
        "        return examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewK9SE3gGEFq"
      },
      "source": [
        "    def get_examples(self, mode):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            mode: train, dev, test\n",
        "        \"\"\"\n",
        "        file_to_read = None\n",
        "        if mode == 'train':\n",
        "            file_to_read = self.args.train_file\n",
        "        elif mode == 'dev':\n",
        "            file_to_read = self.args.dev_file\n",
        "        elif mode == 'test':\n",
        "            file_to_read = self.args.test_file\n",
        "\n",
        "        logger.info(\"LOOKING AT {}\".format(os.path.join(self.args.data_dir,\n",
        "                                                        self.args.task,\n",
        "                                                        file_to_read)))\n",
        "        return self._create_examples(self._read_file(os.path.join(self.args.data_dir,\n",
        "                                                                  self.args.task,\n",
        "                                                                  file_to_read)), mode)\n",
        "logger = logging.getLogger(__name__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qfIKqewvQAPU"
      },
      "source": [
        "\n",
        "def train(args,\n",
        "          model,\n",
        "          train_dataset,\n",
        "          dev_dataset=None,\n",
        "          test_dataset=None):\n",
        "    train_sampler = RandomSampler(train_dataset) #데이터를 섞음\n",
        "    train_dataloader = DataLoader(train_dataset, sampler=train_sampler, batch_size=args.train_batch_size) #batch사이즈만 큼 데이터 load\n",
        "    if args.max_steps > 0:\n",
        "        t_total = args.max_steps\n",
        "        args.num_train_epochs = args.max_steps // (len(train_dataloader) // args.gradient_accumulation_steps) + 1\n",
        "    else:\n",
        "        t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs #?\n",
        "\n",
        "    # Prepare optimizer and schedule (linear warmup and decay)\n",
        "    no_decay = ['bias', 'LayerNorm.weight']\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay': args.weight_decay},\n",
        "        {'params': [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)], \n",
        "         'weight_decay': 0.0}\n",
        "    ]\n",
        "    optimizer = AdamW(optimizer_grouped_parameters, lr=args.learning_rate, eps=args.adam_epsilon)\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=int(t_total * args.warmup_proportion), num_training_steps=t_total)\n",
        "    #learning rate가 올라가다가 떨어지는 지점을 설정\n",
        "    if os.path.isfile(os.path.join(args.model_name_or_path, \"optimizer.pt\")) and os.path.isfile(\n",
        "            os.path.join(args.model_name_or_path, \"scheduler.pt\")\n",
        "    ):\n",
        "        # Load optimizer and scheduler states\n",
        "        optimizer.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"optimizer.pt\")))\n",
        "        scheduler.load_state_dict(torch.load(os.path.join(args.model_name_or_path, \"scheduler.pt\")))\n",
        "\n",
        " # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Num examples = %d\", len(train_dataset))\n",
        "    logger.info(\"  Num Epochs = %d\", args.num_train_epochs)\n",
        "    logger.info(\"  Total train batch size = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "    logger.info(\"  Total optimization steps = %d\", t_total)\n",
        "    logger.info(\"  Logging steps = %d\", args.logging_steps)\n",
        "    logger.info(\"  Save steps = %d\", args.save_steps)\n",
        "\n",
        "    global_step = 0 \n",
        "    tr_loss = 0.0\n",
        "\n",
        "    model.zero_grad()\n",
        "    mb = master_bar(range(int(args.num_train_epochs)))\n",
        "    for epoch in mb:\n",
        "        epoch_iterator = progress_bar(train_dataloader, parent=mb)\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            model.train()\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\" : batch[2], #문장구분 토큰\n",
        "                \"labels\": batch[3],\n",
        "                \"return_dict\": False #\n",
        "            }\n",
        "                  \n",
        "            outputs = model(**inputs)\n",
        "\n",
        "            loss = outputs[0]\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "\n",
        "            loss.backward()\n",
        "            tr_loss += loss.item()\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0 or (\n",
        "                    len(train_dataloader) <= args.gradient_accumulation_steps\n",
        "                    and (step + 1) == len(train_dataloader)\n",
        "            ):\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "\n",
        "                optimizer.step()\n",
        "                scheduler.step()\n",
        "                model.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.logging_steps > 0 and global_step % args.logging_steps == 0:\n",
        "                    if args.evaluate_test_during_training:\n",
        "                        evaluate(args, model, test_dataset, \"test\", global_step)\n",
        "                    else:\n",
        "                        evaluate(args, model, dev_dataset, \"dev\", global_step)\n",
        "\n",
        "                if args.save_steps > 0 and global_step % args.save_steps == 0:\n",
        "                    # Save model checkpoint\n",
        "                    output_dir = os.path.join(args.output_dir, \"checkpoint-{}\".format(global_step))\n",
        "                    if not os.path.exists(output_dir):\n",
        "                        os.makedirs(output_dir)\n",
        "                    model_to_save = (\n",
        "                        model.module if hasattr(model, \"module\") else model\n",
        "                    )\n",
        "                    model_to_save.save_pretrained(output_dir)\n",
        "\n",
        "                    torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
        "                    logger.info(\"Saving model checkpoint to {}\".format(output_dir))\n",
        "\n",
        "                    if args.save_optimizer:\n",
        "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
        "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
        "                        logger.info(\"Saving optimizer and scheduler states to {}\".format(output_dir))\n",
        "\n",
        "            if args.max_steps > 0 and global_step > args.max_steps:\n",
        "                break\n",
        "\n",
        "        mb.write(\"Epoch {} done\".format(epoch + 1))\n",
        "\n",
        "        if args.max_steps > 0 and global_step > args.max_steps:\n",
        "            break\n",
        "\n",
        "    return global_step, tr_loss / global_step\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buAJYmUHRyVm"
      },
      "source": [
        "def evaluate(args, model, eval_dataset, mode, global_step=None):\n",
        "    results = {}\n",
        "    eval_sampler = SequentialSampler(eval_dataset)\n",
        "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=args.eval_batch_size)\n",
        "\n",
        "    # Eval!\n",
        "    if global_step != None:\n",
        "        logger.info(\"***** Running evaluation on {} dataset ({} step) *****\".format(mode, global_step))\n",
        "    else:\n",
        "        logger.info(\"***** Running evaluation on {} dataset *****\".format(mode))\n",
        "    logger.info(\"  Num examples = {}\".format(len(eval_dataset)))\n",
        "    logger.info(\"  Eval Batch size = {}\".format(args.eval_batch_size))\n",
        "    eval_loss = 0.0\n",
        "    nb_eval_steps = 0\n",
        "    preds = None\n",
        "    out_label_ids = None\n",
        "\n",
        "    for batch in progress_bar(eval_dataloader):\n",
        "        model.eval()\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            inputs = {\n",
        "                \"input_ids\": batch[0],\n",
        "                \"attention_mask\": batch[1],\n",
        "                \"token_type_ids\": batch[2],\n",
        "                \"labels\": batch[3],\n",
        "                \"return_dict\": False\n",
        "            }\n",
        "           \n",
        "            outputs = model(**inputs)\n",
        "            tmp_eval_loss, logits = outputs[:2]\n",
        "\n",
        "            eval_loss += tmp_eval_loss.mean().item()\n",
        "        nb_eval_steps += 1\n",
        "        if preds is None:\n",
        "            preds = logits.detach().cpu().numpy()\n",
        "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
        "        else:\n",
        "            preds = np.append(preds, logits.detach().cpu().numpy(), axis=0)\n",
        "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
        "\n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    results = {\n",
        "        \"loss\": eval_loss\n",
        "    }\n",
        "    preds = np.argmax(preds, axis=2)\n",
        "\n",
        "    labels = PROCESSOR(args).get_labels()\n",
        "\n",
        "    label_map = {i: label for i, label in enumerate(labels)}\n",
        "\n",
        "    out_label_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
        "\n",
        "    pad_token_label_id = CrossEntropyLoss().ignore_index\n",
        "\n",
        "    for i in range(out_label_ids.shape[0]):\n",
        "        for j in range(out_label_ids.shape[1]):\n",
        "            if out_label_ids[i, j] != pad_token_label_id:\n",
        "                out_label_list[i].append(label_map[out_label_ids[i][j]])\n",
        "                preds_list[i].append(label_map[preds[i][j]])\n",
        "\n",
        "    result = compute_metrics(out_label_list, preds_list)\n",
        "    results.update(result)\n",
        "\n",
        "    output_dir = os.path.join(args.output_dir, mode)\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    output_eval_file = os.path.join(output_dir, \"{}-{}.txt\".format(mode, global_step) if global_step else \"{}.txt\".format(mode))\n",
        "    with open(output_eval_file, \"w\") as f_w:\n",
        "        logger.info(\"***** Eval results on {} dataset *****\".format(mode))\n",
        "        for key in sorted(results.keys()):\n",
        "            logger.info(\"  {} = {}\".format(key, str(results[key])))\n",
        "            f_w.write(\"  {} = {}\\n\".format(key, str(results[key])))\n",
        "        logger.info(\"\\n\" + show_ner_report(out_label_list, preds_list)) # Show report for each tag result\n",
        "        f_w.write(\"\\n\" + show_ner_report(out_label_list, preds_list))\n",
        "        \n",
        "    return results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NhUHXYTT0JO"
      },
      "source": [
        "def main(config_file):\n",
        "    # Read from config file and make args\n",
        "    with open(config_file) as f:\n",
        "        args = AttrDict(json.load(f))\n",
        "    logger.info(\"Training/evaluation parameters {}\".format(args))\n",
        "\n",
        "    args.output_dir = os.path.join(args.ckpt_dir, args.output_dir)\n",
        "\n",
        "    init_logger()\n",
        "\n",
        "    processor = PROCESSOR(args)\n",
        "    labels = processor.get_labels()\n",
        "    config = CONFIG_CLASS.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        num_labels=TASK_NUM_LABELS,\n",
        "        id2label={str(i): label for i, label in enumerate(labels)},\n",
        "        label2id={label: i for i, label in enumerate(labels)},\n",
        "    )\n",
        "    tokenizer = TOKENIZER_CLASS.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        do_lower_case=args.do_lower_case\n",
        "    )\n",
        "    model = MODEL_FOR_TOKEN_CLASSIFICATION.from_pretrained(\n",
        "        args.model_name_or_path,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    # GPU or CPU\n",
        "    args.device = \"cuda\" if torch.cuda.is_available() and not args.no_cuda else \"cpu\"\n",
        "    model.to(args.device)\n",
        "\n",
        "    # Load dataset\n",
        "    train_dataset = load_and_cache_examples(args, tokenizer, mode=\"train\") if args.train_file else None\n",
        "    dev_dataset = load_and_cache_examples(args, tokenizer, mode=\"dev\") if args.dev_file else None\n",
        "    test_dataset = load_and_cache_examples(args, tokenizer, mode=\"test\") if args.test_file else None\n",
        "\n",
        "    if dev_dataset == None:\n",
        "        args.evaluate_test_during_training = True  # If there is no dev dataset, only use testset\n",
        "\n",
        "    if args.do_train:\n",
        "        global_step, tr_loss = train(args, model, train_dataset, dev_dataset, test_dataset)\n",
        "        logger.info(\" global_step = {}, average loss = {}\".format(global_step, tr_loss))\n",
        "\n",
        "    results = {}\n",
        "    if args.do_eval:\n",
        "        checkpoints = list(\n",
        "            os.path.dirname(c) for c in sorted(glob.glob(args.output_dir + \"/**/\" + \"pytorch_model.bin\", recursive=True))\n",
        "        )\n",
        "        if not args.eval_all_checkpoints:\n",
        "            checkpoints = checkpoints[-1:]\n",
        "        else:\n",
        "            logging.getLogger(\"transformers.configuration_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "            logging.getLogger(\"transformers.modeling_utils\").setLevel(logging.WARN)  # Reduce logging\n",
        "        logger.info(\"Evaluate the following checkpoints: %s\", checkpoints)\n",
        "        for checkpoint in checkpoints:\n",
        "            global_step = checkpoint.split(\"-\")[-1]\n",
        "            model = MODEL_FOR_TOKEN_CLASSIFICATION.from_pretrained(checkpoint)\n",
        "            model.to(args.device)\n",
        "            result = evaluate(args, model, test_dataset, mode=\"test\", global_step=global_step)\n",
        "            result = dict((k + \"_{}\".format(global_step), v) for k, v in result.items())\n",
        "            results.update(result)\n",
        "\n",
        "        output_eval_file = os.path.join(args.output_dir, \"eval_results.txt\")\n",
        "        with open(output_eval_file, \"w\") as f_w:\n",
        "            for key in sorted(results.keys()):\n",
        "                f_w.write(\"{} = {}\\n\".format(key, str(results[key])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvA7FshmUfub"
      },
      "source": [
        "CONFIG_CLASS = ElectraConfig\n",
        "TOKENIZER_CLASS = ElectraTokenizer\n",
        "MODEL_FOR_TOKEN_CLASSIFICATION  = ElectraForTokenClassification\n",
        "TASK_NUM_LABELS = 29 \n",
        "PROCESSOR = NaverNerProcessor"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGk5F_gkVCEz"
      },
      "source": [
        "config_file = '/content/drive/MyDrive/kolectra_ner/koelectra-small-v3.json'\n",
        "main(config_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mam7cdNUZU9A"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pi14MhPwAfDl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mn9_M5mnAfjT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}